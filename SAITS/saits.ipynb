{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b6de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "%pip install \"pandas<3.0.0\"\n",
    "%pip install pypots benchpots pygrinder --upgrade\n",
    "%pip install scikit-learn   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas necessárias para manipulação de dados e pré-processamento\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar, calc_missing_rate\n",
    "from benchpots.datasets import preprocess_physionet2012\n",
    "\n",
    "# Carrega e pré-processa o conjunto de dados PhysioNet 2012\n",
    "# subset='set-a' seleciona o subconjunto A do dataset\n",
    "# rate=0.1 adiciona artificialmente 10% de valores faltantes aos dados para teste\n",
    "data = preprocess_physionet2012(subset='set-a', rate=0.1)\n",
    "\n",
    "# Separa os dados em conjuntos de treino, validação e teste\n",
    "# Cada conjunto contém séries temporais com valores faltantes (originais + artificiais)\n",
    "train_X, val_X, test_X = data[\"train_X\"], data[\"val_X\"], data[\"test_X\"]\n",
    "\n",
    "# Exibe as dimensões dos dados: (n_amostras, n_passos_temporais, n_características)\n",
    "print(train_X.shape)\n",
    "print(val_X.shape)  # O número de amostras varia entre treino e validação, mas passos e características são iguais\n",
    "\n",
    "# Calcula e exibe a taxa de valores faltantes no conjunto de treino\n",
    "print(f\"Temos {calc_missing_rate(train_X):.1%} valores faltantes em train_X\")\n",
    "\n",
    "# Prepara o conjunto de treino: apenas as séries temporais incompletas são necessárias\n",
    "train_set = {\"X\": train_X}\n",
    "\n",
    "# Prepara o conjunto de validação: inclui tanto os dados incompletos quanto os originais completos\n",
    "# X_ori serve como ground truth para avaliar o desempenho do modelo e selecionar o melhor checkpoint\n",
    "val_set = {\n",
    "    \"X\": val_X,\n",
    "    \"X_ori\": data[\"val_X_ori\"],\n",
    "}\n",
    "\n",
    "# Prepara o conjunto de teste: contém apenas as séries temporais incompletas\n",
    "# O modelo irá imputar (preencher) os valores faltantes neste conjunto\n",
    "test_set = {\"X\": test_X}\n",
    "\n",
    "# Obtém os valores originais completos do conjunto de teste para avaliação final\n",
    "test_X_ori = data[\"test_X_ori\"]\n",
    "\n",
    "# Cria uma máscara que identifica os valores que foram artificialmente removidos\n",
    "# Esta máscara indica onde estão os valores que existem em X_ori mas foram removidos em test_X\n",
    "# Usa XOR (^) para encontrar posições que são faltantes em test_X mas presentes em test_X_ori\n",
    "indicating_mask = np.isnan(test_X) ^ np.isnan(test_X_ori)\n",
    "\n",
    "# Importa o modelo SAITS (Self-Attention-based Imputation for Time Series)\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.nn.functional import calc_mae\n",
    "\n",
    "# Configura e inicializa o modelo SAITS com os hiperparâmetros:\n",
    "# - n_steps: número de passos temporais na série\n",
    "# - n_features: número de características/variáveis em cada passo\n",
    "# - n_layers: 2 camadas de transformers\n",
    "# - d_model: 256 dimensões no espaço de embedding\n",
    "# - n_heads: 4 cabeças de atenção paralelas\n",
    "# - d_k, d_v: 64 dimensões para keys e values no mecanismo de atenção\n",
    "# - d_ffn: 128 dimensões na rede feed-forward\n",
    "# - dropout: 0.1 taxa de dropout para regularização\n",
    "# - epochs: 5 épocas de treinamento\n",
    "saits = SAITS(\n",
    "    n_steps=train_X.shape[1],\n",
    "    n_features=train_X.shape[2],\n",
    "    n_layers=2,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    d_ffn=128,\n",
    "    dropout=0.1,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# Treina o modelo usando o conjunto de treino e valida com o conjunto de validação\n",
    "# O modelo aprende a reconstruir valores faltantes através de mecanismos de auto-atenção\n",
    "saits.fit(train_set, val_set)\n",
    "\n",
    "# Usa o modelo treinado para imputar (preencher) todos os valores faltantes no conjunto de teste\n",
    "# Isso inclui tanto os valores originalmente faltantes quanto os artificialmente removidos\n",
    "imputation = saits.impute(test_set)\n",
    "\n",
    "# Calcula o erro absoluto médio (MAE) comparando os valores imputados com os valores verdadeiros\n",
    "# Avalia apenas os valores que foram artificialmente removidos (indicados pela máscara)\n",
    "mae = calc_mae(imputation, np.nan_to_num(test_X_ori), indicating_mask)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Salva o modelo treinado em arquivo para uso futuro\n",
    "# overwrite=True permite sobrescrever o arquivo se ele já existir\n",
    "saits.save(\"save_it_here/saits_physionet2012.pypots\", overwrite=True)\n",
    "\n",
    "# Recarrega o modelo salvo do arquivo\n",
    "# Útil para continuar o treinamento ou realizar novas imputações posteriormente\n",
    "saits.load(\"save_it_here/saits_physionet2012.pypots\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
